[["index.html", "Data Science for Psychologists A Modernized Exploratory and Graphical Data Analysis with R Welcome to PSY 703", " Data Science for Psychologists A Modernized Exploratory and Graphical Data Analysis with R S. Mason Garrison 2021-01-20 Welcome to PSY 703 Welcome to class! This website is designed to accompany Mason Garrisons Data Science for Psychologists (DS4P). DS4P is a graduate-level quantitative methods course at Wake Forest University. This website hosts the lab notes. All the embedded lecture videos can be found on a youtube playlist. You can find the current version of the course syllabus here, along with all of the other syllabi for my classes. "],["course-introduction-module-0.html", "Course Introduction: Module 0 Big Ideas Materials Knowledge is Power Course Modality Attribution Colophon License", " Course Introduction: Module 0 This overview is designed to orient you to the class. Please watch the videos from this playlist and work your way through the notes. Although the module-level playlists are embedded in the course, you can find the full-course video playlist here. Data Science for Psychologists (DS4P) introduces on the principles of data science, including: data wrangling, modeling, visualization, and communication. In this class, we link those principles to psychological methods and open science practices by emphasizing exploratory analyses and description, rather than confirmatory analyses and prediction. Through the semester we will work our way thru Wickham and Grolemunds R for Data Science text and develop proficiency with tidyverse. This class emphasizes replication and reproducibility. DS4P is a practical skilled-based class and should be useful to students aiming for academia as well as those interested in industry. Applications of these methods can be applied to a full range of psychological areas, including perception (e.g, eye-tracking data), neuroscience (e.g., visualizing neural networks), and individual differences (e.g., valence analysis). Big Ideas This class covers the following broad five areas: Reproducibility; Replication; Robust Methods; Resplendent Visualizations; and R Programming. Materials Hardware This class is requires that you have a laptop that can run R. Required Texts The text is intended to supplement the videos, lecture notes, and in-class tutorials. You need to consume all four in order to be successful in this class. R for Data Science text Software R and RStudio R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows, and MacOS. RStudio is a free integrated development environment (IDE), a powerful user interface for R. Git and Github Git is a version control system. Its original purpose was to help groups of developers work collaboratively on big software projects. Git manages the evolution of a set of files  called a repository  in a structured way. Think of it like the Track Changes features from Microsoft Word. Github is a free IDE and hosting service for Git. As a Wake Forest student, you should be able to access the GitHub Student Developer Pack for free. It includes a free PRO upgrade for your github account Knowledge is Power This brief video is covers the icebreaker I do in all of my classes. I encourage you to watch it. In it, I discuss stereotype threats and statistics anxiety. Course Modality This class is a blended class. The online portions are asynchronous. Ive created a video highlighting how to be a successful asynchronous learner. Much of this information comes from Northeastern Universitys Tips for Taking Online Classes Attribution This class leans heavily on other peoples materials and ideas. I have done my best to document the origin of the materials and ideas. They include: Jenny Bryans (jennybryan.org) STAT 545; Joe Rodgerss PSY 8751 Exploratory and Graphical Data Analysis Course Julia Fukuyamas EXPLORATORY DATA ANALYSIS Mine Çetinkaya-Rundels Data Science in a Box. You can see specific changes by examining the edit history on the git repo Colophon This book was written in bookdown inside RStudio. The website r-computing-lab.github.io/DataScience4Psych is hosted with github, The complete source is available from GitHub. The Psych 703 logo was designed by me and the book style was designed by Desirée De Leon. This version of the book was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.0.3 (2020-10-10) #&gt; os Windows 10 x64 #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.1252 #&gt; ctype English_United States.1252 #&gt; tz America/New_York #&gt; date 2021-01-20 Along with these packages: License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Sharecopy and redistribute the material in any medium or format Remixremix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: AttributionYou must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlikeIf you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictionsYou may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["welcome-to-data-science-module-01.html", "1 Welcome to Data Science: Module 01 1.1 Module Materials 1.2 What is Data Science? 1.3 Course structure and some other useful things 1.4 Meet our toolbox!", " 1 Welcome to Data Science: Module 01 This module is designed to introduce you to data science. Please watch the videos and work your way through the notes. You can find the module playlist here. Most of the slides used to make the videos in this module can be found in the slides repo. 1.1 Module Materials Slides Welcome Slides Meet the toolkit Activities UN Voting Covid Data Bechdal Test Lab [Hello R] 1.2 What is Data Science? You can follow along with the slides here if they do not appear below. Ive embedded a few examples below. 1.2.1 Hans Rosling The below video is the shorter version. Hans Roslings 200 Countries, 200 Years, 4 Minutes - The Joy of Stats You can find a longer talk-length version below. The video included in this tweet is enthusiastic about data science. YASSSSSSSSSS MY LOVE STEVE IS BACK!!! #KornackiThirstcontinues pic.twitter.com/ynK4D87Bhr&mdash; Leslie Jones  (@Lesdoggg) January 5, 2021 1.3 Course structure and some other useful things You can follow along with the slides here if they do not appear below. 1.3.1 Activity 01 You can do either activity. The choice is yours. 1.3.1.1 UN Votes You can find the materials for the UN activity here. The compiled version should look something like the following 1.3.1.2 Covid Data You can find the materials for the Covid version of this activity here. The compiled version should look something like the following 1.4 Meet our toolbox! You can follow along with the slides here if they do not appear below. 1.4.1 Activity 02: Bechdel You can find the materials for the Bechdel activity here. The compiled version should look something like the following 1.4.2 Install R and RStudio library(vembedr) embed_url(&quot;https://www.youtube.com/watch?v=kVIZGCT5p9U&quot;) %&gt;% use_align(&quot;center&quot;) Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system  use the links up at the top of the CRAN page linked above! Install RStudios IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop. You can run either the Preview version or the official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. RStudio can interface with Git(Hub). However, you must do all the Git(Hub) set up described elsewhere before you can take advantage of this. If you have a pre-existing installation of R and/or RStudio, I highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new. If you upgrade R, you will need to update any packages you have installed. The command below should get you started, though you may need to specify more arguments if, e.g., you have been using a non-default library for your packages. update.packages(ask = FALSE, checkBuilt = TRUE) Note: this code will only look for updates on CRAN. So if you use a package that lives only on GitHub or if you want a development version from GitHub, you will need to update manually, e.g. via devtools::install_github(). 1.4.2.1 Testing testing Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you havent written any code or made any figures yet! Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x &lt;- 3 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 12 print to screen. If yes, youve succeeded in installing R and RStudio. 1.4.2.2 Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others). install.packages(&quot;dplyr&quot;, dependencies = TRUE) By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around. You could use the above method to install the following packages, all of which we will use: tidyr, package webpage ggplot2, package webpage 1.4.2.3 Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further. How to Use RStudio RStudios leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual 1.4.3 Install Git and Github "],["lab-01-hello-r.html", "2 Lab 01 - Hello R! 2.1 Getting started 2.2 Warm up 2.3 The data 2.4 Exercises", " 2 Lab 01 - Hello R! R is the name of the programming language itself and RStudio is a convenient interface. The main goal of this lab is to introduce you to R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions. git is a version control system (like &quot;Track Changes&quot; features from Microsoft Word on steroids) and GitHub is the home for your Git-based projects on the internet (like DropBox but much, much better). An additional goal is to introduce you to git and GitHub, which is the collaboration and version control system that we will be using throughout the course. As the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today, we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands. To make versioning simpler, this lab is a solo lab. Additionally, we want to make sure everyone gets a significant amount of time at the steering wheel. Next week youll learn about collaborating on GitHub and produce a single lab report for your team. 2.1 Getting started Each of your assignments will begin with the following steps. You saw these once in class yesterday, theyre outlined in detail here again. Going forward each lab will start with a Getting started section but details will be a bit more sparse than this. You can always refer back to this lab for a detailed list of the steps involved for getting started with an assignment. The following screencast also walks you through these steps: Click on the assignment link that you should have received in your email to create your GitHub repository (which well refer to as repo going forward) for the assignment. This repo contains a template you can build on to complete your assignment. On GitHub, click on the green Clone or download button, select Use HTTPS (this might already be selected by default, and if it is, youll see the text Clone with HTTPS as in the image below). Click on the clipboard icon to copy the repo URL. Go to RStudio Cloud and into the course workspace. Create a New Project from Git Repo. You will need to click on the down arrow next to the New Project button to see this option. Copy and paste the URL of your assignment repo into the dialog box: Hit OK, and youre good to go! 2.1.1 Packages In this lab we will work with two packages: datasauRus which contains the dataset, and tidyverse which is a collection of packages for doing data analysis in a tidy way. Install these packages by running the following in the console. install.packages(&quot;tidyverse&quot;) install.packages(&quot;datasauRus&quot;) Now that the necessary packages are installed, you should be able to Knit your document and see the results. If youd like to run your code in the Console as well youll also need to load the packages there. To do so, run the following in the console. library(tidyverse) library(datasauRus) Note that the packages are also loaded with the same commands in your R Markdown document. 2.1.2 Housekeeping Your email address is the address tied to your GitHub account and your name should be first and last name. Before we can get started we need to take care of some required housekeeping. Specifically, we need to configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your email address and your name. To do so, run the following: usethis::use_git_config(user.name = &quot;your name&quot;, user.email = &quot;your email&quot;) For example, for me this looks like: usethis::use_git_config(user.name = &quot;Mason Garrison&quot;, user.email = &quot;garrissm@wfu.edu&quot;) 2.2 Warm up Before we introduce the data, lets warm up with some simple exercises. The following video is an overview of some of these warm-up exercises. 2.2.1 Project name Currently your project is called Untitled Project. Update the name of your project to be Lab 01 - Hello R. The top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for &quot;YAML Ain&#39;t Markup Language&quot;. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document. 2.2.2 YAML Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. 2.2.3 Committing changes Then go to the Git pane in your RStudio. If you have made changes to your Rmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes. If youre happy with these changes, write Update author name in the Commit message box and hit Commit. You dont have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions. 2.2.4 Pushing changes Now that you have made an update and committed this change, its time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course teaching team (your repos in this course are private to you and us, only). In order to push your changes to GitHub, click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. This might feel cumbersome. Bear with me We will teach you how to save your password so you dont have to enter it every time. But for this one assignment youll have to manually enter each time you push in order to gain some experience with it. 2.2.5 Thought exercise For which of the above steps (changing project name, making updates to the document, committing, and pushing changes) do you need to have an internet connection? Discuss with your classmates. 2.3 The data If it&#39;s confusing that the data frame is called `datasaurus_dozen` when it contains 13 datasets, you&#39;re not alone! Have you heard of a [baker&#39;s dozen](https://en.wikipedia.org/wiki/Dozen#Baker&#39;s_dozen)? The data frame we will be working with today is called datasaurus_dozen and its in the datasauRus package. Actually, this single data frame contains 13 datasets, designed to show us why data visualisation is important and how summary statistics alone can be misleading. The different datasets are marked by the dataset variable. To find out more about the dataset, type the following in your Console: ?datasaurus_dozen. A question mark before the name of an object will always bring up its help file. This command must be ran in the Console. 2.4 Exercises Based on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report. When youre done, commit your changes with the commit message Added answer for Ex 1, and push. Lets take a look at what these datasets are. To do so we can make a frequency table of the dataset variable: datasaurus_dozen %&gt;% count(dataset) %&gt;% print(13) ## # A tibble: 13 x 2 ## dataset n ## &lt;chr&gt; &lt;int&gt; ## 1 away 142 ## 2 bullseye 142 ## 3 circle 142 ## 4 dino 142 ## 5 dots 142 ## 6 h_lines 142 ## 7 high_lines 142 ## 8 slant_down 142 ## 9 slant_up 142 ## 10 star 142 ## 11 v_lines 142 ## 12 wide_lines 142 ## 13 x_shape 142 Matejka, Justin, and George Fitzmaurice. &quot;Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing.&quot; Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 2017. The original Datasaurus (dino) was created by Alberto Cairo in this great blog post. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice. In the paper, the authors simulate a variety of datasets that have the same summary statistics as the Datasaurus but have very different distributions. Plot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for this dataset. Below is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your Rmd document and successfully knit it and view the results. Start with the datasaurus_dozen and pipe it into the filter function to filter for observations where dataset == \"dino\". Store the resulting filtered data frame as a new data frame called dino_data. dino_data &lt;- datasaurus_dozen %&gt;% filter(dataset == &quot;dino&quot;) There is a lot going on here, so lets slow down and unpack it a bit. First, the pipe operator: %&gt;%, takes what comes before it and sends it as the first argument to what comes after it. So here, were saying filter the datasaurus_dozen data frame for observations where dataset == \"dino\". Second, the assignment operator: &lt;-, assigns the name dino_data to the filtered data frame. Next, we need to visualize these data. We will use the ggplot function for this. Its first argument is the data youre visualizing. Next we define the aesthetic mappings. In other words, the columns of the data that get mapped to certain aesthetic features of the plot, e.g. the x axis will represent the variable called x and the y axis will represent the variable called y. Then, we add another layer to this plot where we define which geometric shapes we want to use to represent each observation in the data. In this case we want these to be points, hence geom_point. ggplot(data = dino_data, mapping = aes(x = x, y = y)) + geom_point() If this seems like a lot, it is. And you will learn about the philosophy of building data visualizations in layer in detail next week. For now, follow along with the code that is provided. For the second part of these exercises, we need to calculate a summary statistic: the correlation coefficient. Correlation coefficient, often referred to as \\(r\\) in statistics, measures the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This is exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesnt make sense since the relationship between x and y is definitely not linear  its dinosaurial! But, for illustrative purposes, lets calculate the correlation coefficient between x and y. Start with `dino_data` and calculate a summary statistic that we will call `r` as the `cor`relation between `x` and `y`. dino_data %&gt;% summarize(r = cor(x, y)) ## # A tibble: 1 x 1 ## r ## &lt;dbl&gt; ## 1 -0.0645 This is a good place to pause, commit changes with the commit message Added answer for Ex 2, and push. Plot y vs. x for the star dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino? This is another good place to pause, commit changes with the commit message Added answer for Ex 3, and push. Plot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino? You should pause again, commit changes with the commit message Added answer for Ex 4, and push. Facet by the dataset variable, placing the plots in a 3 column grid, and don&#39;t add a legend. Finally, lets plot all datasets at once. In order to do this we will make use of facetting. ggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset))+ geom_point()+ facet_wrap(~ dataset, ncol = 3) + theme(legend.position = &quot;none&quot;) And we can use the group_by function to generate all the summary correlation coefficients. datasaurus_dozen %&gt;% group_by(dataset) %&gt;% summarize(r = cor(x, y)) %&gt;% print(13) Youre done with the data analysis exercises, but wed like you to do two more things: Resize your figures: Click on the gear icon in on top of the R Markdown document, and select Output Options in the dropdown menu. In the pop up dialogue box go to the Figures tab and change the height and width of the figures, and hit OK when done. Then, knit your document and see how you like the new sizes. Change and knit again and again until youre happy with the figure sizes. Note that these values get saved in the YAML. You can also use different figure sizes for differen figures. To do so click on the gear icon within the chunk where you want to make a change. Changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your R Markdown document as well. Change the look of your report: Once again click on the gear icon in on top of the R Markdown document, and select Output Options in the dropdown menu. In the General tab of the pop up dialogue box try out different Syntax highlighting and theme options. Hit OK and knit your document to see how it looks. Play around with these until youre happy with the look. Not sure how to use emojis on your computer? Maybe a teammate can help? Or you can ask your TA as well! Yay, youre done! Commit all remaining changes, use the commit message Done with Lab 1! , and push. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo. "],["data-and-visualization-module-2.html", "3 Data and Visualization: Module 2 3.1 Module Materials 3.2 Activity 03: Star Wars!", " 3 Data and Visualization: Module 2 This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 3.1 Module Materials Slides Data and visualization Building plots for various data types Activities Star Wars! Lab [Plastic waste] 3.2 Activity 03: Star Wars! You can find the materials for the activity here. The compiled version should look something like the following "],["lab-02-global-plastic-waste.html", "4 Lab 02 - Global plastic waste 4.1 Getting started 4.2 The data 4.3 Exercises", " 4 Lab 02 - Global plastic waste **A note on expectations: ** For each exercise and on your own question you answer include any relevant output (tables, summary statistics, plots) in your answer. Doing this is easy! Just place any relevant R code in a code chunk, and hit Knit HTML. Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010. Additionally, National Geographic recently ran a data visualization communication contest on plastic waste as seen here. Learning goals for this lab are: Visualize numerical and categorical data. Recreate visualizations. Get more practice using with Git and GitHub. 4.1 Getting started **IMPORTANT:** If there is no GitHub repo created for you for this assignment, it means I didn&#39;t have your GitHub username as of when I assigned the homework. Please let me know your GitHub username asap, and I can create your repo. Go to the course GitHub organization and locate your Lab 02 repo, which should be named lab-02-plastic-waste-YOUR_GITHUB_USERNAME. Grab the URL of the repo, and clone it in RStudio. Refer to Lab 01 if you would like to see step-by-step instructions for cloning a repo into an RStudio project. First, open the R Markdown document lab-02-plastic-waste.Rmd and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. 4.1.1 Packages Well use the tidyverse package for this analysis. Run the following code in the Console to load this package. library(tidyverse) 4.1.2 Housekeeping Your email address is the address tied to your GitHub account and your name should be first and last name. Before we can get started we need to take care of some required housekeeping. Specifically, we need to configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your email address and your name. To do so, run the following: usethis::use_git_config(user.name = &quot;your name&quot;, user.email = &quot;your email&quot;) For example, for me this looks like: usethis::use_git_config(user.name = &quot;Mine Cetinkaya-Rundel&quot;, user.email = &quot;cetinkaya.mine@gmail.com&quot;) 4.2 The data The dataset for this assignment can be found as a csv file in the data folder of your repository. You can read it in using the following. plastic_waste &lt;- read_csv(&quot;data/plastic-waste.csv&quot;) The variable descriptions are as follows: code: 3 Letter country code entity: Country name continent: Continent name year: Year gdp_per_cap: GDP per capita constant 2011 international $, rate plastic_waste_per_cap: Amount of plastic waste per capita in kg/day mismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day mismanaged_plastic_waste: Tonnes of mismanaged plastic waste coastal_pop: Number of individuals living on/near coast total_pop: Total population according to Gapminder 4.3 Exercises Lets start by taking a look at the distribution of plastic waste per capita in 2010. ggplot(data = plastic_waste, aes(x = plastic_waste_per_cap)) + geom_histogram(binwidth = 0.2) ## Warning: Removed 51 rows containing non-finite values (stat_bin). One country stands out as an unusual observation at the top of the distribution. One way of identifying this country is to filter the data for countries where plastic waste per capita is greater than 3.5 kg/person. plastic_waste %&gt;% filter(plastic_waste_per_cap &gt; 3.5) ## # A tibble: 1 x 10 ## code entity continent year gdp_per_cap plastic_waste_p~ mismanaged_plas~ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TTO Trini~ North Am~ 2010 31261. 3.6 0.19 ## # ... with 3 more variables: mismanaged_plastic_waste &lt;dbl&gt;, coastal_pop &lt;dbl&gt;, ## # total_pop &lt;dbl&gt; Did you expect this result? You might consider doing some research on Trinidad and Tobago to see why plastic waste per capita is so high there, or whether this is a data error. Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? From this point onwards the plots / output of the code won&#39;t be printed in the lab, but you can run the code and view the results yourself. Another way of visualizing numerical data is using density plots. ggplot(data = plastic_waste, aes(x = plastic_waste_per_cap)) + geom_density() And compare distributions across continents by coloring density curves by continent. ggplot(data = plastic_waste, mapping = aes(x = plastic_waste_per_cap, color = continent)) + geom_density() The resulting plot may be a little difficult to read, so lets also fill the curves in with colors as well. ggplot(data = plastic_waste, mapping = aes(x = plastic_waste_per_cap, color = continent, fill = continent)) + geom_density() The overlapping colors make it difficult to tell whats happening with the distributions in continents plotted first, and hence coverred by continents plotted over them. We can change the transparency level of the fill color to help with this. The alpha argument takes values between 0 and 1: 0 is completely transparent and 1 is completely opaque. There is no way to tell what value will work best, so you just need to try a few. ggplot(data = plastic_waste, mapping = aes(x = plastic_waste_per_cap, color = continent, fill = continent)) + geom_density(alpha = 0.7) This still doesnt look great Recreate the density plots above using a different (lower) alpha level that works better for displaying the density curves for all continents. Describe why we defined the color and fill of the curves by mapping aesthetics of the plot but we defined the alpha level as a characteristic of the plotting geom.   Now is a good time to commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. And yet another way to visualize this relationship is using side-by-side box plots. ggplot(data = plastic_waste, mapping = aes(x = continent, y = plastic_waste_per_cap)) + geom_boxplot() Learn something new: violin plots! Read about them at http://ggplot2.tidyverse.org/reference/geom_violin.html, and convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots? Remember that we use `geom_point()` to make scatterplots. Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship. Color the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated? Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. Do either of these pairs of variables appear to be more strongly linearly associated?   Now is another good time to commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. Hint: The x-axis is a calculated variable. One country with plastic waste per capita over 3 kg/day has been filtered out. And the colors are from the viridis color palette. Take a look at the functions starting with `scale_color_viridis_*`. Recreate the following plot, and interpret what you see in context of the data.   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. "],["tidy-data-and-data-wrangling-module-3.html", "5 Tidy data and data wrangling: Module 3 5.1 Module Materials 5.2 Activity 03: Star Wars! (UPDATE)", " 5 Tidy data and data wrangling: Module 3 This module is designed to &lt;&gt;. Please watch the videos and work your way through the notes. You can find the video playlist for this module here. Most of the slides used to make the videos in this module can be found in the slides repo. 5.1 Module Materials Slides Tidy data and data wrangling Joining data from multiple sources Activities (UPDATE) Star Wars! Lab (UPDATE) [Plastic waste] 5.2 Activity 03: Star Wars! (UPDATE) You can find the materials for the activity here. The compiled version should look something like the following "],["lab-03-nobel-laureates.html", "6 Lab 03 - Nobel laureates 6.1 Workflow 6.2 Getting started 6.3 The data 6.4 Exercises 6.5 Wrapping up 6.6 Interested in how Buzzfeed made their visualizations?", " 6 Lab 03 - Nobel laureates In January 2017, Buzzfeed published an article on why Nobel laureates show immigration is so important for American science. You can read the article here. In the article they show that while most living Nobel laureates in the sciences are based in the US, many of them were born in other countries. This is one reason why scientific leaders say that immigration is vital for progress. In this lab we will work with the data from this article to recreate some of their visualizations as well as explore new questions. The learning goals of this lab are: Manipulate and transform data to prepare it for visualization. Recreate visualizations. Summarise data. Get more practice working as a team. 6.1 Workflow This is the first week youre working in teams. You have a team repository that each member of the team has access to. You can all push to this repository, but for this week only we will keep things simple and ask that the team lead for the week is the only one who pushes while others work on together with them. Starting next week well show you how you can all collectively work in a repo, the (mini) chaos that might result in (called a merge conflict), and how to resolve it. 6.2 Getting started Go to the course GitHub organization and locate your Lab 03 repo, which should be named lab-03-nobel-winners-YOUR_TEAMNAME. Grab the URL of the repo, and clone it in RStudio. Refer to Lab 01 if you would like to see step-by-step instructions for cloning a repo into an RStudio project. First, open the R Markdown document lab-03-nobel-winners.Rmd and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. 6.2.1 Packages Well use the tidyverse package for this analysis. Run the following code in the Console to load this package. library(tidyverse) 6.3 The data The dataset for this assignment can be found as a csv file in the data folder of your repository. You can read it in using the following. nobel &lt;- read_csv(&quot;data/nobel.csv&quot;) The variable descriptions are as follows: id: ID number firstname: First name of laureate surname: Surname year: Year prize won category: Category of prize affiliation: Affiliation of laureate city: City of laureate in prize year country: Country of laureate in prize year born_date: Birth date of laureate died_date: Death date of laureate gender: Gender of laureate born_city: City where laureate was born born_country: Country where laureate was born born_country_code: Code of country where laureate was born died_city: City where laureate died died_country: Country where laureate died died_country_code: Code of country where laureate died overall_motivation: Overall motivation for recognition share: Number of other winners award is shared with motivation: Motivation for recognition In a few cases the name of the city/country changed after laureate was given (e.g. in 1975 Bosnia and Herzegovina was part of the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix _original. born_country_original: Original country where laureate was born born_city_original: Original city where laureate was born died_country_original: Original country where laureate died died_city_original: Original city where laureate died city_original: Original city where laureate lived at the time of winning the award country_original: Original country where laureate lived at the time of winning the award 6.4 Exercises 6.4.1 Get to know your data How many observations and how many variables are in the dataset? Use inline code to answer this question. There are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results. Create a new data frame called nobel_living that filters for laureates for whom country is available laureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender) laureates who are still alive (their died_date is NA) Confirm that once you have filtered for these characteristics you are left with a data frame with 228 observations.   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 6.4.2 Most living Nobel laureates were based in the US when they won their prizes  says the Buzzfeed article. Lets see if thats true. First, well create a new variable to identify whether the laureate was in the US when they won their prize. Well use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function were using to write this if statement is the condition were testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\". Note that we can achieve the same result using the `fct_other()` function we&#39;ve seen before (i.e. with `country_us = fct_other(country, &quot;USA&quot;)`). We decided to use the `if_else()` here to show you one example of an if statement in R. nobel_living &lt;- nobel_living %&gt;% mutate( country_us = if_else(country == &quot;USA&quot;, &quot;USA&quot;, &quot;Other&quot;) ) Next, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics. nobel_living_science &lt;- nobel_living %&gt;% filter(category %in% c(&quot;Physics&quot;, &quot;Medicine&quot;, &quot;Chemistry&quot;, &quot;Economics&quot;)) For the next exercise work with the nobel_living_science data frame you created above. This means youll need to define this data frame in your R Markdown document, even though the next exercise doesnt explicitly ask you to do so. Create a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Note: Your visualization should be faceted by category. For each facet you should have two bars, one for winners in the US and one for Other. Flip the coordinates so the bars are horizontal, not vertical. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data.   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 6.4.3 But of those US-based Nobel laureates, many were born in other countries **Hint:** You should be able to ~~cheat~~ borrow from code you used earlier to create the `country_us` variable. Create a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. Add a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Your final visualization should contain a facet for each category, within each facet a bar for whether they won the award in the US or not, and within each bar whether they were born in the US or not. Based on your visualization, do the data appear to support Buzzfeeds claim? Explain your reasoning in 1-2 sentences.   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 6.4.4 Heres where those immigrant Nobelists were born Note that your bar plot won&#39;t exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published. In a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count()) function for their birth country, born_country, and arrange the resulting data frame in descending order of number of observations for each country.   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 6.5 Wrapping up Go back through your write up to make sure youre following coding style guidelines we discussed in class. Make any edits as needed. Also, make sure all of your R chunks are properly labeled, and your figures are reasonably sized. Once the team leader for the week pushes their final changes, others should also clone the team repo (or if youve already done so, pull on the Git pane) and knit the R Markdown document to confirm that they can reproduce the report. 6.6 Interested in how Buzzfeed made their visualizations? The plots in the Buzzfeed article are called waffle plots. You can find the code used for making these plots in Buzzfeeds GitHub repo (yes, they have one!) here. Youre not expected to recreate them as part of your assignment, but youre welcomed to do so for fun! "],["la-quinta-is-spanish-for-next-to-dennys-pt-1.html", "7 La Quinta is Spanish for next to Dennys, Pt. 1 7.1 Visualizing spatial data 7.2 Getting started 7.3 Housekeeping 7.4 The data 7.5 Exercises", " 7 La Quinta is Spanish for next to Dennys, Pt. 1 7.1 Visualizing spatial data Have you ever taken a road trip in the US and thought to yourself I wonder what La Quinta means. Well, the late comedian Mitch Hedberg thinks its Spanish for next to Dennys. If youre not familiar with these two establishments, Dennys is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain. These two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data. The inspiration for this lab comes from a blog post by John Reiser on his new jersey geographer blog. You can read that analysis here. Reisers blog post focuses on scraping data from Dennys and La Quinta Inn and Suites websites using Python. In this lab we focus on visualization and analysis of these data. However note that the data scraping was also done in R, and we we will discuss web scraping using R later in the course. But for now we focus on the data that has already been scraped and tidied for you. 7.2 Getting started 7.2.1 Packages In this lab we will use the tidyverse and dsbox packages. library(tidyverse) library(dsbox) 7.3 Housekeeping 7.3.1 Project name Currently your project is called Untitled Project. Update the name of your project to be Lab 03 - Visualizing spatial data. 7.3.2 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 7.3.3 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 7.3.4 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 7.3.5 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 7.4 The data The datasets well use are called dennys and laquinta from the dsbox package. Note that these data were scraped from here and here, respectively. To help with our analysis we will also use a dataset on US states: states &lt;- read_csv(&quot;data/states.csv&quot;) Each observation in this dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles). 7.5 Exercises What are the dimensions of the Dennys dataset? (Hint: Use inline R code and functions like nrow and ncol to compose your answer.) What does each row in the dataset represent? What are the variables? What are the dimensions of the La Quintas dataset? What does each row in the dataset represent? What are the variables? We would like to limit our analysis to Dennys and La Quinta locations in the United States. Take a look at the websites that the data come from (linked above). Are there any La Quintas locations outside of the US? If so, which countries? What about Dennys? Now take a look at the data. What would be some ways of determining whether or not either establishment has any locations outside the US using just the data (and not the websites). Dont worry about whether you know how to implement this, just brainstorm some ideas. Write down at least one as your answer, but youre welcomed to write down a few options too. We will determine whether or not the establishment has a location outside the US using the state variable in the dn and lq datasets. We know exactly which states are in the US, and we have this information in the states dataframe we loaded. Find the Dennys locations that are outside the US, if any. To do so, filter the Dennys locations for observations where state is not in states$abbreviation. The code for this is given below. Note that the %in% operator matches the states listed in the state variable to those listed in states$abbreviation. The ! operator means not. Are there any Dennys locations outside the US? &quot;Filter for `state`s that are not in `states$abbreviation`.&quot; dn %&gt;% filter(!(state %in% states$abbreviation)) Add a country variable to the Dennys dataset and set all observations equal to \"United States\". Remember, you can use the mutate function for adding a variable. Make sure to save the result of this as dn again so that the stored data frame contains the new variable going forward. We don&#39;t need to tell R how many times to repeat the character string &quot;United States&quot; to fill in the data for all observations, R takes care of that automatically. dn %&gt;% mutate(country = &quot;United States&quot;) Find the La Quinta locations that are outside the US, and figure out which country they are in. This might require some googling. Take notes, you will need to use this information in the next exercise. Add a country variable to the La Quinta dataset. Use the case_when function to populate this variable. Youll need to refer to your notes from Exercise 7 about which country the non-US locations are in. Here is some starter code to get you going: lq %&gt;% mutate(country = case_when( state %in% state.abb ~ &quot;United States&quot;, state %in% c(&quot;ON&quot;, &quot;BC&quot;) ~ &quot;Canada&quot;, state == &quot;ANT&quot; ~ &quot;Colombia&quot;, ... # fill in the rest )) Going forward we will work with the data from the United States only. All Dennys locations are in the United States, so we dont need to worry about them. However we do need to filter the La Quinta dataset for locations in United States. lq &lt;- lq %&gt;% filter(country == &quot;United States&quot;) Which states have the most and fewest Dennys locations? What about La Quinta? Is this surprising? Why or why not? Next, lets calculate which states have the most Dennys locations per thousand square miles. This requires joinining information from the frequency tables you created in Exercise 8 with information from the states data frame. First, we count how many observations are in each state, which will give us a data frame with two variables: state and n. Then, we join this data frame with the states data frame. However note that the variables in the states data frame that has the two-letter abbreviations is called abbreviation. So when were joining the two data frames we specify that the state variable from the Dennys data should be matched by the abbreviation variable from the states data: dn %&gt;% count(state) %&gt;% inner_join(states, by = c(&quot;state&quot; = &quot;abbreviation&quot;)) Before you move on the the next question, run the code above and take a look at the output. In the next exercise you will need to build on this pipe. Which states have the most Dennys locations per thousand square miles? What about La Quinta? Next, we put the two datasets together into a single data frame. However before we do so, we need to add an identifier variable. Well call this establishment and set the value to \"Denny's\" and \"La Quinta\" for the dn and lq data frames, respectively. dn &lt;- dn %&gt;% mutate(establishment = &quot;Denny&#39;s&quot;) lq &lt;- lq %&gt;% mutate(establishment = &quot;La Quinta&quot;) Since the two data frames have the same columns, we can easily bind them with the bind_rows function: dn_lq &lt;- bind_rows(dn, lq) We can plot the locations of the two establishments using a scatter plot, and color the points by the establishment type. Note that the latitude is plotted on the x-axis and the longitude on the y-axis. ggplot(dn_lq, mapping = aes(x = longitude, y = latitude, color = establishment)) + geom_point() The following two questions ask you to create visualizations. These should follow best practices you learned in class, such as informative titles, axis labels, etc. See http://ggplot2.tidyverse.org/reference/labs.html for help with the syntax. You can also choose different themes to change the overall look of your plots, see http://ggplot2.tidyverse.org/reference/ggtheme.html for help with these. Filter the data for observations in North Carolina only, and recreate the plot. You should also adjust the transparency of the points, by setting the alpha level, so that its easier to see the overplotted ones. Visually, does Mitch Hedbergs joke appear to hold here? Now filter the data for observations in Texas only, and recreate the plot, with an appropriate alpha level. Visually, does Mitch Hedbergs joke appear to hold here? Thats it for now! In the next lab we will take a more quantitative approach to answering these questions. "],["lab-05-la-quinta-is-spanish-for-next-to-dennys-pt-2.html", "8 Lab 05 - La Quinta is Spanish for next to Dennys, Pt. 2\" 8.1 Wrangling spatial data 8.2 Getting started 8.3 Warm up 8.4 The data 8.5 Exercises", " 8 Lab 05 - La Quinta is Spanish for next to Dennys, Pt. 2\" 8.1 Wrangling spatial data In this lab we revisit the Dennys and La Quinta Inn and Suites data we visualized in the previous lab. 8.2 Getting started Go to the course organization on GitHub. Find your lab repo. In the repo, click on the green Clone or download button, select Use HTTPS (this might already be selected by default, and if it is, youll see the text Clone with HTTPS as in the image below). Click on the clipboard icon to copy the repo URL. Go to RStudio Cloud and into the course workspace. Create a New Project from Git Repo. You will need to click on the down arrow next to the New Project button to see this option. Copy and paste the URL of your assignment repo into the dialog box: Hit OK, and youre good to go! 8.2.1 Packages In this lab we will use the tidyverse and dsbox packages. library(tidyverse) library(dsbox) 8.2.2 Housekeeping 8.2.2.1 Password caching If you would like your git password cached for a week for this project, type the following in the Terminal: git config --global credential.helper &#39;cache --timeout 604800&#39; 8.2.2.2 Project name Currently your project is called Untitled Project. Update the name of your project to be Lab 04 - Wrangling spatial data. 8.3 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 8.3.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 8.3.2 Commiting and pushing changes: Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 8.3.3 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 8.4 The data The datasets well use are called dennys and laquinta from the dsbox package. 8.5 Exercises Filter the Dennys dataframe for Alaska (AK) and save the result as dn_ak. How many Dennys locations are there in Alaska? dn_ak &lt;- dn %&gt;% filter(state == &quot;AK&quot;) nrow(dn_ak) Filter the La Quinta dataframe for Alaska (AK) and save the result as lq_ak. How many La Quinta locations are there in Alaska? lq_ak &lt;- lq %&gt;% filter(state == &quot;AK&quot;) nrow(lq_ak) Next well calculate the distance between all Dennys and all La Quinta locations in Alaska. Lets take this step by step: Step 1: There are 3 Dennys and 2 La Quinta locations in Alaska. (If you answered differently above, you might want to recheck your answers.) Step 2: Lets focus on the first Dennys location. Well need to calculate two distances for it: (1) distance between Dennys 1 and La Quinta 1 and (2) distance between Dennys 1 and La Quinta (2). Step 3: Now lets consider all Dennys locations. How many pairings are there between all Dennys and all La Quinta locations in Alaska, i.e. how many distances do we need to calculate between the locations of these establishments in Alaska? In order to calculate these distances we need to first restructure our data to pair the Dennys and La Quinta locations. To do so, we will join the two data frames. We have six join options in R. Each of these join functions take at least three arguments: x, y, and by. x and y are data frames to join by is the variable(s) to join by Four of these join functions combine variables from the two data frames: These are called **mutating joins**. inner_join(): return all rows from x where there are matching values in y, and all columns from x and y. left_join(): return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. right_join(): return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. full_join(): return all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing. And the other two join functions only keep cases from the left-hand data frame, and are called filtering joins. Well learn about these another time but you can find out more about the join functions in the help files for any one of them, e.g. ?full_join. In practice we mostly use mutating joins. In this case we want to keep all rows and columns from both dn_ak and lq_ak data frames. So we will use a full_join. Full join of Dennys and La Quinta locations in AK Lets join the data on Dennys and La Quinta locations in Alaska, and take a look at what it looks like: dn_lq_ak &lt;- full_join(dn_ak, lq_ak, by = &quot;state&quot;) dn_lq_ak How many observations are in the joined dn_lq_ak data frame? What are the names of the variables in this data frame. .x in the variable names means the variable comes from the x data frame (the first argument in the full_join call, i.e. dn_ak), and .y means the variable comes from the y data frame. These varibles are renamed to include .x and .y because the two data frames have the same variables and its not possible to have two variables in a data frame with the exact same name. Now that we have the data in the format we wanted, all that is left is to calculate the distances between the pairs. What function from the tidyverse do we use the add a new variable to a data frame while keeping the existing variables? One way of calculating the distance between any two points on the earth is to use the Haversine distance formula. This formula takes into account the fact that the earth is not flat, but instead spherical. This function is not available in R, but we have it saved in a file called haversine.R that we can load and then use: haversine &lt;- function(long1, lat1, long2, lat2, round = 3) { # convert to radians long1 = long1 * pi / 180 lat1 = lat1 * pi / 180 long2 = long2 * pi / 180 lat2 = lat2 * pi / 180 R = 6371 # Earth mean radius in km a = sin((lat2 - lat1)/2)^2 + cos(lat1) * cos(lat2) * sin((long2 - long1)/2)^2 d = R * 2 * asin(sqrt(a)) return( round(d,round) ) # distance in km } This function takes five arguments: Longitude and latitude of the first location Longitude and latitude of the second location A parameter by which to round the responses Calculate the distances between all pairs of Dennys and La Quinta locations and save this variable as distance. Make sure to save this variable in THE dn_lq_ak data frame so that you can use it later. Calculate the minimum distance between a Dennys and La Quinta for each Dennys location. To do so we group by Dennys locations and calculate a new variable that stores the information for the minimum distance. dn_lq_ak_mindist &lt;- dn_lq_ak %&gt;% group_by(address.x) %&gt;% summarise(closest = min(distance)) Describe the distribution of the distances Dennys and the nearest La Quinta locations in Alaska. Also include an appripriate visualization and relevant summary statistics. Repeat the same analysis for North Carolina: (i) filter Dennys and La Quinta Data Frames for NC, (ii) join these data frames to get a completelist of all possible pairings, (iii) calculate the distances between all possible pairings of Dennys and La Quinta in NC, (iv) find the minimum distance between each Dennys and La Quinta location, (v) visualize and describe the distribution of these shortest distances using appropriate summary statistics. Repeat the same analysis for Texas. Repeat the same analysis for a state of your choosing, different than the ones we covered so far. Among the states you examined, where is Mitch Hedbergs joke most likely to hold true? Explain your reasoning. "],["lab-06-ugly-charts.html", "9 Lab 06 - Ugly charts 9.1 Getting started 9.2 Merges and merge conflicts 9.3 Packages 9.4 Take a sad plot and make it better 9.5 Wrapping up 9.6 More ugly charts", " 9 Lab 06 - Ugly charts Given below are two data visualizations that violate many data visualization best practices. Improve these visualizations using R and the tips for effective visualizations that we introduced in class. You should produce one visualization per dataset. Your visualization should be accompanied by a brief paragraph describing the choices you made in your improvement, specifically discussing what you didnt like in the original plots and why, and how you addressed them in the visualization you created. On the due date you will give a brief presentation describing one of your improved visualizations and the reasoning for the choices you made. The learning goals for this lab are: Telling a story with data Data visualization best practices Reshaping data 9.1 Getting started Go to the course GitHub organization and locate your lab repo. Grab the URL of the repo, and clone it in RStudio. Refer to Lab 01 if you would like to see step-by-step instructions for cloning a repo into an RStudio project. First, open the R Markdown document and Knit it. Make sure it compiles without errors. The output will be in the file markdown .md file with the same name. 9.1.1 Housekeeping Your email address is the address tied to your GitHub account and your name should be first and last name. Before we can get started we need to take care of some required housekeeping. Specifically, we need to do some configuration so that RStudio can communicate with GitHub. This requires two pieces of information: your email address and your name. Run the following (but update it for your name and email!) in the Console to configure git: library(usethis) use_git_config(user.name = &quot;Your Name&quot;, user.email = &quot;your.email@address.com&quot;) 9.1.2 Workflow This is the second week youre working in teams, so were going to make things a little more interesting and let all of you make changes and push those changes to your team repository. Sometimes things will go swimmingly, and sometimes youll run into merge conflicts. So our first task today is to walk you through a merge conflict! 9.2 Merges and merge conflicts Pushing to a repo replaces the code on GitHub with the code you have on your computer. If a collaborator has made a change to your repo on GitHub that you havent incorporated into your local work, GitHub will stop you from pushing to the repo because this could overwrite your collaborators work! So you need to explicitly merge your collaborators work before you can push. If your and your collaborators changes are in different files or in different parts of the same file, git merges the work for you automatically when you pull. If you both changed the same part of a file, git will produce a merge conflict because it doesnt know how which change you want to keep and which change you want to overwrite. Git will put conflict markers in your code that look like: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD See also: [dplyr documentation](https://dplyr.tidyverse.org/) ======= See also [ggplot2 documentation](https://ggplot2.tidyverse.org/) &gt;&gt;&gt;&gt;&gt;&gt;&gt; some1alpha2numeric3string4 The ===s separate your changes (top) from their changes (bottom). Note that on top you see the word HEAD, which indicates that these are your changes. And at the bottom you see some1alpha2numeric3string4 (well, it probably looks more like 28e7b2ceb39972085a0860892062810fb812a08f). This is the hash (a unique identifier) of the commit your collaborator made with the conflicting change. Your job is to reconcile the changes: edit the file so that it incorporates the best of both versions and delete the &lt;&lt;&lt;, ===, and &gt;&gt;&gt; lines. Then Stage and Commit the result. 9.2.1 Setup Clone the repo and open the .Rmd file. Assign the numbers 1, 2, 3, and 4 to each of the team members. If your team has fewer than 4 people, some people will need to have multiple numbers. If your team has more than 4 people, some people will need to share some numbers. 9.2.2 Lets cause a merge conflict! Our goal is to see two different types of merges: first well see a type of merge that git cant figure out on its own how to do on its own (a merge conflict) and requires human intervention, then another type of where that git can figure out how to do without human intervention. Doing this will require some tight choreography, so pay attention! Take turns in completing the exercise, only one member at a time. Others should just watch, not doing anything on their own projects (this includes not even pulling changes!) until they are instructed to. If you feel like you wont be able to resist the urge to touch your computer when its not your turn, we recommend putting your hands in your pockets or sitting on them! Before starting: everyone should have the repo cloned and know which role number(s) they are. Role 1: Change the team name to your actual team name. Knit, commit, push.  Wait for instructions before moving on to the next step. Role 2: Change the team name to some other word. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by editing the document to choose the correct/preferred change. Knit. Click the Stage checkbox for all files in your Git tab. Make sure they all have check marks, not filled-in boxes. Commit and push.  Wait for instructions before moving on to the next step. Role 3: Add a label to the first code chunk Knit, commit, push. You should get an error. Pull. No merge conflicts should occur, but you should see a message about merging. Now push.  Wait for instructions before moving on to the next step. Role 4: Add a different label to the first code chunk. Knit, commit, push. You should get an error. Pull. Take a look at the document with the merge conflict. Clear the merge conflict by choosing the correct/preferred change. Commit, and push.  Wait for instructions before moving on to the next step. Everyone: Pull, and observe the changes in your document. 9.2.3 Tips for collaborating via GitHub Always pull first before you start working. Resolve a merge conflict (commit and push) before continuing your work. Never do new work while resolving a merge conflict. Knit, commit, and push often to minimize merge conflicts and/or to make merge conflicts easier to resolve. If you find yourself in a situation that is difficult to resolve, ask questions ASAP. Dont let it linger and get bigger. 9.3 Packages Run the following code in the Console to load this package. library(tidyverse) 9.4 Take a sad plot and make it better 9.4.1 Instructional staff employment trends The American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report compiled by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains an image very similar to the one given below. Lets start by loading the data used to create this plot. staff &lt;- read_csv(&quot;data/instructional-staff.csv&quot;) Each row in this dataset represents a faculty type, and the columns are the years for which we have data. The values are percentage of hires of that type of faculty for each year. ## # A tibble: 5 x 12 ## faculty_type `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Full-Time T~ 29 27.6 25 24.8 21.8 20.3 19.3 17.8 17.2 ## 2 Full-Time T~ 16.1 11.4 10.2 9.6 8.9 9.2 8.8 8.2 8 ## 3 Full-Time N~ 10.3 14.1 13.6 13.6 15.2 15.5 15 14.8 14.9 ## 4 Part-Time F~ 24 30.4 33.1 33.2 35.5 36 37 39.3 40.5 ## 5 Graduate St~ 20.5 16.5 18.1 18.8 18.7 19 20 19.9 19.5 ## # ... with 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt; In order to recreate this visualization we need to first reshape the data to have one variable for faculty type and one variable for year. In other words, we will convert the data from wide format to long format. But before we do so, a thought exercise: How many rows will the long-format data have? It will have a row for each combination of year and faculty type. If there are 5 faculty types and 11 years of data, how many rows will we have? We do the wide to long conversion using a new function: pivot_longer(). The animation below show how this function works, as well as its counterpart pivot_wider(). The function has the following arguments: pivot_longer(data, cols, names_to = &quot;name&quot;) The first argument is data as usual. The second argument, cols, is where you specify which columns to pivot into longer format  in this case all columns except for the faculty_type The third argument, names_to, is a string specifying the name of the column to create from the data stored in the column names of data  in this case year staff_long &lt;- staff %&gt;% pivot_longer(cols = -faculty_type, names_to = &quot;year&quot;) %&gt;% mutate(value = as.numeric(value)) Lets take a look at what the new longer data frame looks like. staff_long ## # A tibble: 55 x 3 ## faculty_type year value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Full-Time Tenured Faculty 1975 29 ## 2 Full-Time Tenured Faculty 1989 27.6 ## 3 Full-Time Tenured Faculty 1993 25 ## 4 Full-Time Tenured Faculty 1995 24.8 ## 5 Full-Time Tenured Faculty 1999 21.8 ## 6 Full-Time Tenured Faculty 2001 20.3 ## 7 Full-Time Tenured Faculty 2003 19.3 ## 8 Full-Time Tenured Faculty 2005 17.8 ## 9 Full-Time Tenured Faculty 2007 17.2 ## 10 Full-Time Tenured Faculty 2009 16.8 ## # ... with 45 more rows And now lets plot is as a line plot. A possible approach for creating a line plot where we color the lines by faculty type is the following: staff_long %&gt;% ggplot(aes(x = year, y = value, color = faculty_type)) + geom_line() ## geom_path: Each group consists of only one observation. Do you need to adjust ## the group aesthetic? But note that this results in a message as well as an unexpected plot. The message is saying that there is only one observation for each faculty type year combination. We can fix this using the group aesthetic following. staff_long %&gt;% ggplot(aes(x = year, y = value, group = faculty_type, color = faculty_type)) + geom_line() Include the line plot you made above in your report and make sure the figure width is large enough to make it legible. Also fix the title, axis labels, and legend label. Suppose the objective of this plot was to show that the proportion of part-time faculty have gone up over time compared to other instructional staff types. What changes would you propose making to this plot to tell this story. (You dont need to implement these changes now, you will get to do that as part of this weeks homework. But work as a team to come up with ideas and list them as bullet points. The more precise you are, the easier your homework will be.)   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 9.4.2 Fisheries Fisheries and Aquaculture Department of the Food and Agriculture Organization of the United Nations collects data on fisheries production of countries. This Wikipedia page lists fishery production of countries for 2016. For each country tonnage from capture and aquaculture are listed. Note that countries whose total harvest was less than 100,000 tons are not included in the visualization. A researcher shared with you the following visualization they created based on these data . Can you help them make improve it? First, brainstorm how you would improve it. Then create the improved visualization and write up the changes/decisions you made as bullet points. Its ok if some of your improvements are aspirational, i.e. you dont know how to implement it, but you think its a good idea. Ask a tutor for help, but also keep an eye on the time. Implement what you can and leave note identifying the aspirational improvements. fisheries &lt;- read_csv(&quot;data/fisheries.csv&quot;)   Commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 9.5 Wrapping up Go back through your write up to make sure youre following coding style guidelines we discussed in class. Make any edits as needed. Also, make sure all of your R chunks are properly labeled, and your figures are reasonably sized. Once the team leader for the week pushes their final changes, others should pull the changes and knit the R Markdown document to confirm that they can reproduce the report. 9.6 More ugly charts Want to see more ugly charts? Flowing Data - Ugly Charts Reddit - Data is ugly Missed Opportunities and Graphical Failures (Mostly Bad) Graphics and Tables "],["lab-07-smokers-in-whickham.html", "10 Lab 07 Smokers in Whickham 10.1 Simpsons paradox 10.2 Getting started 10.3 The data 10.4 Exercises", " 10 Lab 07 Smokers in Whickham 10.1 Simpsons paradox 10.2 Getting started A study of conducted in Whickham, England recorded participants age, smoking status at baseline, and then 20 years later recorded their health outcome. 10.2.1 Packages In this lab we will work with the tidyverse and mosaicData packages. This is the first time were using the mosaicData package, you need to make sure to install it first by running install.packages(\"mosaicData\") in the console. library(tidyverse) library(mosaicData) Note that these packages are also loaded in your R Markdown document. 10.3 The data The data is in the mosaicData package. You can load it with data(Whickham) Take a peek at the codebook with ?Whickham 10.4 Exercises What type of study do you think these data comne from: observational or experiment? Why? How many observations are in this dataset? What does each observation represent? How many variables are in this dataset? What type of variable is each? Display each variable using an appropriate visualization. What would you expect the relationship between smoking status and health outcome to be? Create a visualization depicting the relationship between smoking status and health outcome. Briefly describe the relationship, and evaluate whether this meets your expectations. Additionally, calculate the relevant conditional probabilities to help your narrative. Here is some code to get you started: Whickham %&gt;% count(smoker, outcome) Create a new variable called age_cat using the following scheme: age &lt;= 44 ~ \"18-44\" age &gt; 44 &amp; age &lt;= 64 ~ \"45-64\" age &gt; 64 ~ \"65+\" Re-create the visualization depicting the relationship between smoking status and health outcome, faceted by age_cat. What changed? What might explain this change? Extend the contingency table from earlier by breaking it down by age category and use it to help your narrative. Whickham %&gt;% count(smoker, age_cat, outcome) "],["lab-08-university-of-edinburgh-art-collection.html", "11 Lab 08 - University of Edinburgh Art Collection 11.1 Getting started 11.2 R scripts vs. R Markdown documents 11.3 SelectorGadget 11.4 Functions 11.5 Iteration 11.6 Analysis", " 11 Lab 08 - University of Edinburgh Art Collection The University of Edinburgh Art Collection supports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.1. See the sidebar [here](https://collections.ed.ac.uk/art) and note that there are 2909 pieces in the art collection we&#39;re collecting data on. In this lab well scrape data on all art pieces in the Edinburgh College of Art collection. The earning goals of this lab are: Web scraping from a single page Writing functions Iteration Writing data Before getting started, lets check that a bot has permissions to access pages on this domain. paths_allowed(&quot;https://collections.ed.ac.uk/art)&quot;) ## collections.ed.ac.uk ## [1] TRUE 11.1 Getting started Go to the course GitHub organization and locate your lab repo, which should be named lab-08-uoe-art-YOUR_TEAMNAME. Grab the URL of the repo, and clone it in RStudio Cloud. Your email address is the address tied to your GitHub account and your name should be first and last name. Run the following (but update it for your name and email!) in the Console to configure Git: library(usethis) use_git_config(user.name = &quot;Your Name&quot;, user.email = &quot;your.email@address.com&quot;) 11.2 R scripts vs. R Markdown documents Today we will be using both R scripts and R Markdown documents: .R: R scripts are plain text files containing only code and brief comments, Well use R scripts in the web scraping stage and ultimately save the scraped data as a csv. .Rmd: R Markdown documents are plain text files containing. Well use an R Markdown document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage. Here is the organization of your repo, and the corresponding section in the lab that each file will be used for: |-data | |- README.md |-lab-06-uoe-art.Rmd # analysis |-lab-06-uoe-art.Rproj |-README.md |-scripts # webscraping | |- 01-scrape-page-one.R # scraping a single page | |- 02-scrape-page-function.R # functions | |- 03-scrape-page-many.R # iteration 11.3 SelectorGadget For this lab, please use Google Chrome as your web browser. If you are using the one of the computers in the computer lab, you can access Google Chrome by searching for it in the search bar at the bottom left of your home screen. Then, go to the SelectorGadget extension page on the Chrome Web Store and click on Add to Chrome (big blue button). A pop up window will ask Add SelectorGadget?, click Add extension. Another pop up window will asl whether you want to get your extensions on all your computer. If you want this, you can turn on sync, but you dont need to for the purpose of this lab. You should now be able to access SelectorGadget by clicking on the icon next to the search bar in the Chrome browser. 11.3.1 Scraping a single page **Tip:** To run the code you can highlight or put your cursor next to the lines of code you want to run and hit Command+Enter. Work in scripts/01-scrape-page-one.R. We will start off by scraping data on the first 10 pieces in the collection from here. First, we define a new object called first_url, which is the link above. Then, we read the page at this url with the read_html() function from the rvest package. The code for this is already provided in 01-scrape-page-one.R. # set url first_url &lt;- &quot;https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0&quot; # read html page page &lt;- read_html(first_url) For the ten pieces on this page we will extract title, artist, and link information, and put these three variables in a data frame. 11.3.2 Titles Lets start with titles. We make use of the SelectorGadget to identify the tags for the relevant nodes: page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) ## {xml_nodeset (10)} ## [1] &lt;a href=&quot;./record/112340?highlight=*:*&quot;&gt;untitled ... ## [2] &lt;a href=&quot;./record/112355?highlight=*:*&quot;&gt;In&#39;&#39;Head ... ## [3] &lt;a href=&quot;./record/112351?highlight=*:*&quot;&gt;Anatomy Test Jeckon H ... ## [4] &lt;a href=&quot;./record/112354?highlight=*:*&quot;&gt;Untitled ... ## [5] &lt;a href=&quot;./record/112357?highlight=*:*&quot;&gt;untitled ... ## [6] &lt;a href=&quot;./record/112350?highlight=*:*&quot;&gt;Untitled ... ## [7] &lt;a href=&quot;./record/112352?highlight=*:*&quot;&gt;Untitled ... ## [8] &lt;a href=&quot;./record/112353?highlight=*:*&quot;&gt;Untitled ... ## [9] &lt;a href=&quot;./record/112345?highlight=*:*&quot;&gt;Untitled ... ## [10] &lt;a href=&quot;./record/112344?highlight=*:*&quot;&gt;Untitled ... Then we extract the text with html_text(): page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() ## [1] &quot;untitled (1984)&quot; ## [2] &quot;In&#39;&#39;Head (1973)&quot; ## [3] &quot;Anatomy Test Jeckon H (2019)&quot; ## [4] &quot;Untitled (2019)&quot; ## [5] &quot;untitled (2019)&quot; ## [6] &quot;Untitled (2019)&quot; ## [7] &quot;Untitled (2019)&quot; ## [8] &quot;Untitled (2019)&quot; ## [9] &quot;Untitled (2019)&quot; ## [10] &quot;Untitled (2019)&quot; And get rid of all the spurious whitespace in the text with str_squish(): Take a look at the help docs for `str_squish()` (with `?str_squish`) to page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() %&gt;% str_squish() ## [1] &quot;untitled (1984)&quot; &quot;In&#39;&#39;Head (1973)&quot; ## [3] &quot;Anatomy Test Jeckon H (2019)&quot; &quot;Untitled (2019)&quot; ## [5] &quot;untitled (2019)&quot; &quot;Untitled (2019)&quot; ## [7] &quot;Untitled (2019)&quot; &quot;Untitled (2019)&quot; ## [9] &quot;Untitled (2019)&quot; &quot;Untitled (2019)&quot; And finally save the resulting data as a vector of length 10: titles &lt;- page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% html_node(&quot;h3 a&quot;) %&gt;% html_text() %&gt;% str_squish() 11.3.3 Links The same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title. We can extract this information using a new function from the rvest package, html_attr(), which extracts attributes. A mini HTML lesson! The following is how we define hyperlinked text in HTML: &lt;a href=&quot;https://www.google.com&quot;&gt;Seach on Google&lt;/a&gt; And this is how the text would look like on a webpage: Seach on Google. Here the text is Seach on Google and the href attribute contains the url of the website youd go to if you click on the hyperlinked text: https://www.google.com. The moral of the story is: the link is stored in the href attribute. page %&gt;% html_nodes(&quot;.iteminfo&quot;) %&gt;% # same nodes html_node(&quot;h3 a&quot;) %&gt;% # as before html_attr(&quot;href&quot;) # but get href attribute instead of text ## [1] &quot;./record/112340?highlight=*:*&quot; &quot;./record/112355?highlight=*:*&quot; ## [3] &quot;./record/112351?highlight=*:*&quot; &quot;./record/112354?highlight=*:*&quot; ## [5] &quot;./record/112357?highlight=*:*&quot; &quot;./record/112350?highlight=*:*&quot; ## [7] &quot;./record/112352?highlight=*:*&quot; &quot;./record/112353?highlight=*:*&quot; ## [9] &quot;./record/112345?highlight=*:*&quot; &quot;./record/112344?highlight=*:*&quot; These dont really look like urls as we know then though. Theyre relative links. See the help for `str_replace()` to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the `pattern` and `replacement` arguments. Click on one of art piece titles in your browser and take note of the url of the webpage it takes you to. How does that url compare to what we scraped above? How is it different? Using str_replace(), fix the URLs. 11.3.4 Artists Fill in the blanks to scrape artist names. 11.3.5 Put it altogether Fill in the blanks to organize everything in a tibble. 11.3.6 Scrape the next page Click on the next page, and grab its url. Fill in the blank in to define a new object: second_url. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as second_ten. 11.4 Functions Work in scripts/02-scrape-page-function.R. Youve been using R functions, now its time to write your own! Lets start simple. Here is a function that takes in an argument x, and adds 2 to it. add_two &lt;- function(x){ x + 2 } Lets test it: add_two(3) ## [1] 5 add_two(10) ## [1] 12 The skeleton for defining functions in R is as follows: function_name &lt;- function(input){ # do something with the input(s) # return something } Then, a function for scraping a page should look something like: **Reminder:** Function names should be short but evocative verbs. function_name &lt;- function(url){ # read page at url # extract title, link, artist info for n pieces on page # return a n x 3 tibble } Fill in the blanks using code you already developed in the previous exercises. Name the function scrape_page. Test out your new function by running the following in the console. Does the output look right? Discuss with teammaates whether youre getting the same results as before. scrape_page(first_url) scrape_page(second_url) 11.5 Iteration Work in scripts/03-scrape-page-many.R. We went from manually scraping individual pages to writing a function to do the same. Next, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection. **Reminder:** The collection has 2909 pieces in total. That means we give develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the scrape_page() function to each page, and combines the resulting data frames from each page into a single data frame with 2909 rows and 3 columns. 11.5.1 List of URLs Click through the first few of the pages in the art collection and observe their URLs to confirm the following pattern: [sometext]offset=0 # Pieces 1-10 [sometext]offset=10 # Pieces 11-20 [sometext]offset=20 # Pieces 21-30 [sometext]offset=30 # Pieces 31-40 ... [sometext]offset=2900 # Pieces 2900-2909 We can construct these URLs in R by pasting together two pieces: (1) a common (root) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 2900. Two new functions are helpful for accomplishing this: paste0() for pasting two pieces of text and seq() for generating a sequence of numbers. Fill in the blanks to construct the list of URLs. 11.5.2 Mapping Finally, were ready to iterate over the list of URLs we constructed. We will do this by mapping the function we developed over the list of URLs. There are a series of mapping functions in R (which well learn about in more detail tomorrow), and they each take the following form: map([x], [function to apply to each element of x]) In our case x is the list of URLs we constructed and the function to apply to each element of x is the function we developed earlier, scrape_page. And as a result we want a data frame, so we use map_dfr function: map_dfr(urls, scrape_page) Fill in the blanks to scrape all pages, and to create a new data frame called uoe_art. 11.5.3 Write out data Finally write out the data frame you constructed into the data folder so that you can use it in the analysis section. 11.6 Analysis Work in lab-06-uoe-art.Rmd for the rest of the lab. Now that we have a tidy dataset that we can analyze, lets do that! Well start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses. Some of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses. This should be interesting to clean up! First thing well try is to separate the title column into two: one for the actual title and the other for the date if it exists. In human speak, we need to separate the title column at the first occurence of ( and put the contents on one side of the ( into a column called title and the contents on the other side into a column called date Luckily, theres a function that does just this: separate()! And once we have completed separating the single title column into title and date, we need to do further cleanup in the date column to get rid of extraneous )s with str_remove(), capture year information, and save the data as a numeric variable. **Hint:** Remember escaping special characters from yesterday&#39;s lecture? You&#39;ll need to use that trick again. Fill in the blanks in to implement the data wrangling we described above. Note that this will result in some warnings when you run the code, and thats OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture year where its convenient to do so. Print out a summary of the dataframe using the skim() function. How many pieces have artist info missing? How many have year info missing? Make a histogram of years. Use a reasonable binwidth. Do you see anything out of the ordinary? **Hint:** You&#39;ll want to use `mutate()` and `if_else()` or `case_when()` to implement the correction. Find which piece has the out of the ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didnt capture the correct year information? Correct the error in the data frame and visualize the data again. Who is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them? **Hint:** You&#39;ll want to use a combination of `filter()` and `str_detect()`. You will want to read the help for `str_detect()` at a minimum, and consider how you might capture titles where the word appears as &quot;child&quot; and &quot;Child&quot;. Final question! How many art pieces have the word child in their title? See if you can figure it out, and ask for help if not. Source: https://collections.ed.ac.uk/art/about "],["lab-09-professor-attractiveness-and-course-evaluations-pt-1.html", "12 Lab 09 - Professor attractiveness and course evaluations, Pt 1 12.1 Modelling with a single predictor 12.2 Getting started 12.3 Warm up 12.4 The data 12.5 Exercises", " 12 Lab 09 - Professor attractiveness and course evaluations, Pt 1 12.1 Modelling with a single predictor 12.2 Getting started Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings. (Daniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. http://www.sciencedirect.com/science/article/pii/S0272775704001165.) For this assignment you will analyze the data from this study in order to learn what goes into a positive professor evaluation. The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors physical appearance. (This is a slightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors. 12.2.1 Packages In this lab we will work with the tidyverse, openintro, and broom packages. library(tidyverse) library(broom) library(openintro) 12.2.2 Housekeeping 12.2.2.1 Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. 12.2.2.2 Project name Update the name of your project to match the labs title. 12.3 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 12.3.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 12.3.2 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 12.3.3 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 12.4 The data The dataset well be using is called evals from the openintro package. Take a peek at the codebook with ?evals. 12.5 Exercises 12.5.1 Part 1: Exploratory Data Analysis Visualize the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not? Include any summary statistics and visualizations you use in your response. Visualize and describe the relationship between score and the new variable you created, bty_avg. **Hint:** See the help page for the function at http://ggplot2.tidyverse.org/reference/index.html. Replot the scatterplot from Exercise 3, but this time use geom_jitter()? What does jitter mean? What was misleading about the initial scatterplot? 12.5.2 Part 2: Linear regression with a numerical predictor Linear model is in the form $\\hat{y} = b_0 + b_1 x$. Lets see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor evaluation score by average beauty rating (bty_avg). Based on the regression output, write the linear model. Replot your visualization from Exercise 3, and add the regression line to this plot in orange color. Turn off the shading for the uncertainty of the line. Interpret the slope of the linear model in context of the data. Interpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context. Determine the \\(R^2\\) of the model and interpret it in context of the data. 12.5.3 Part 3: Linear regression with a categorical predictor Fit a new linear model called m_gen to predict average professor evaluation score based on gender of the professor. Based on the regression output, write the linear model and interpret the slope and intercept in context of the data. What is the equation of the line corresponding to male professors? What is it for female professors? Fit a new linear model called m_rank to predict average professor evaluation score based on rank of the professor. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Create a new variable called rank_relevel where \"tenure track\" is the baseline level. Fit a new linear model called m_rank_relevel to predict average professor evaluation score based on rank_relevel of the professor. This is the new (releveled) variable you created in Exercise 13. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model. Create another new variable called tenure_eligible that labels \"teaching\" faculty as \"no\" and labels \"tenure track\" and \"tenured\" faculty as \"yes\". Fit a new linear model called m_tenure_eligible to predict average professor evaluation score based on tenure_eligibleness of the professor. This is the new (regrouped) variable you created in Exercise 15. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model. "],["lab-10-professor-attractiveness-and-course-evaluations-pt-2.html", "13 Lab 10 - Professor attractiveness and course evaluations, Pt. 2\" 13.1 Modelling with multiple predictors\" 13.2 Getting started 13.3 Warm up 13.4 The data 13.5 Exercises", " 13 Lab 10 - Professor attractiveness and course evaluations, Pt. 2\" 13.1 Modelling with multiple predictors\" In this lab we revisit the professor evaluations data we modeled in the previous lab. In the last lab we modeled evaluation scores using a single predictor at a time. However this time we use multiple predictors to model evaluation scores. If you dont remember the data, review the previous labs introduction before continuing to the exercises. 13.2 Getting started 13.2.1 Packages In this lab we will work with the tidyverse, openintro, and broom packages. library(tidyverse) library(broom) library(openintro) 13.2.2 Housekeeping 13.2.2.1 Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. 13.2.2.2 Project name Update the name of your project to match the labs title. 13.3 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 13.3.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 13.3.2 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 13.3.3 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 13.4 The data The dataset well be using is called evals from the openintro package. Take a peek at the codebook with ?evals. 13.5 Exercises Load the data by including the appropriate code in your R Markdown file. 13.5.1 Part 1: Simple linear regression Fit a linear model (one you have fit before): m_bty, predicting average professor evaluation score based on average beauty rating (bty_avg) only. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\). 13.5.2 Part 2: Multiple linear regression Fit a linear model (one you have fit before): m_bty_gen, predicting average professor evaluation score based on average beauty rating (bty_avg) and gender. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\). Interpret the slope and intercept of m_bty_gen in context of the data. What percent of the variability in score is explained by the model m_bty_gen. What is the equation of the line corresponding to just male professors? For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score? How does the relationship between beauty and evaluation score vary between male and female professors? How do the adjusted \\(R^2\\) values of m_bty_gen and m_bty compare? What does this tell us about how useful gender is in explaining the variability in evaluation scores when we already have information on the beaty score of the professor. Compare the slopes of bty_avg under the two models (m_bty and m_bty_gen). Has the addition of gender to the model changed the parameter estimate (slope) for bty_avg? Create a new model called m_bty_rank with gender removed and rank added in. Write the equation of the linear model and interpret the slopes and intercept in context of the data. 13.5.3 Part 3: The search for the best model Going forward, only consider the following variables as potential predictors: rank, ethnicity, gender, language, age, cls_perc_eval, cls_did_eval, cls_students, cls_level, cls_profs, cls_credits, bty_avg. Which variable, on its own, would you expect to be the worst predictor of evaluation scores? Why? Hint: Think about which variable would you expect to not have any association with the professors score. Check your suspicions from the previous exercise. Include the model output for that variable in your response. Suppose you wanted to fit a full model with the variables listed above. If you are already going to include cls_perc_eval and cls_students, which variable should you not include as an additional predictor? Why? Fit a full model with all predictors listed above (except for the one you decided to exclude) in the previous question. Using backward-selection with adjusted R-squared as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on. Interpret the slopes of one numerical and one categorical predictor based on your final model. Based on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score. Would you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not? "],["lab-11-so-what-if-you-smoke-when-pregnant.html", "14 Lab 11 - So what if you smoke when pregnant?\" 14.1 Simulation based inference 14.2 Getting started 14.3 Housekeeping 14.4 Warm up 14.5 Set a seed! 14.6 The data 14.7 Exercises", " 14 Lab 11 - So what if you smoke when pregnant?\" 14.1 Simulation based inference In 2004, the state of North Carolina released a large data set containing information on births recorded in this state. This data set is useful to researchers studying the relation between habits and practices of expectant mothers and the birth of their children. We will work with a random sample of observations from this data set. 14.2 Getting started 14.2.1 Packages In this lab we will work with the tidyverse, infer, and openintro packages. We can install and load them with the following: library(tidyverse) library(infer) library(openintro) 14.3 Housekeeping 14.3.1 Git configuration / password caching Configure your Git user name and email. If you cannot remember the instructions, refer to an earlier lab. Also remember that you can cache your password for a limited amount of time. 14.3.2 Project name Update the name of your project to match the labs title. 14.4 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, lets warm up with some simple exercises. 14.4.1 YAML Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 14.4.2 Commiting and pushing changes Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like Update team name in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 14.4.3 Pulling changes Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 14.5 Set a seed! In this lab well be generating random samples. The last thing you want is those samples to change every time you knit your document. So, you should set a seed. Theres an R chunk in your R Markdown file set aside for this. Locate it and add a seed. Make sure all members in a team are using the same seed so that you dont get merge conflicts and your results match up for the narratives. 14.6 The data Load the ncbirths data from the openintro package: data(ncbirths) We have observations on 13 different variables, some categorical and some numerical. The meaning of each variable is as follows. variable description fage fathers age in years. mage mothers age in years. mature maturity status of mother. weeks length of pregnancy in weeks. premie whether the birth was classified as premature (premie) or full-term. visits number of hospital visits during pregnancy. marital whether mother is married or not married at birth. gained weight gained by mother during pregnancy in pounds. weight weight of the baby at birth in pounds. lowbirthweight whether baby was classified as low birthweight (low) or not (not low). gender gender of the baby, female or male. habit status of the mother as a nonsmoker or a smoker. whitemom whether mom is white or not white. 14.7 Exercises What are the cases in this data set? How many cases are there in our sample? The first step in the analysis of a new dataset is getting acquanted with the data. Make summaries of the variables in your dataset, determine which variables are categorical and which are numerical. For numerical variables, are there outliers? If you arent sure or want to take a closer look at the data, make a graph. 14.7.1 Baby weights Wen, Shi Wu, Michael S. Kramer, and Robert H. Usher. &quot;Comparison of birth weight distributions between Chinese and Caucasian infants.&quot; American Journal of Epidemiology 141.12 (1995): 1177-1187. A 1995 study suggestes that average weight of Caucasian babies born in the US is 3,369 grams (7.43 pounds). In this dataset we only have information on mothers race, so we will make the simplifying assumption that babies of Caucasian mothers are also Caucasian, i.e. whitemom = \"white\". We want to evaluate whether the average weight of Caucasian babies has changed since 1995. Our null hypothesis should state there is nothing going on, i.e. no change since 1995: \\(H_0: \\mu = 7.43~pounds\\). Our alternative hypothesis should reflect the research question, i.e. some change since 1995. Since the research question doesnt state a direction for the change, we use a two sided alternative hypothesis: \\(H_A: \\mu \\ne 7.43~pounds\\). Create a filtered data frame called ncbirths_white that contain data only from white mothers. Then, calculate the mean of the weights of their babies. Are the conditions necessary for conducting simulation based inference satisfied? Explain your reasoning. Lets discuss how this test would work. Our goal is to simulate a null distribution of sample means that is centered at the null value of 7.43 pounds. In order to do so, we take a bootstrap sample of from the original sample, calculate this bootstrap samples mean, repeat these two steps a large number of times to create a bootstrap distribution of means centered at the observed sample mean, shift this distribution to be centered at the null value by substracting / adding X to all boostrap mean (X = difference between mean of bootstrap distribution and null value), and calculate the p-value as the proportion of bootstrap samples that yielded a sample mean at least as extreme as the observed sample mean. Run the appropriate hypothesis test, visualize the null distribution, calculate the p-value, and interpret the results in context of the data and the hypothesis test. 14.7.2 Baby weight vs. smoking Consider the possible relationship between a mothers smoking habit and the weight of her baby. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions. Make side-by-side boxplots displaying the relationship between habit and weight. What does the plot highlight about the relationship between these two variables? Before moving forward, save a version of the dataset omitting observations where there are NAs for habit. You can call this version ncbirths_habitgiven. The box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the habit variable, and then calculate the mean weight in these groups using. ncbirths_habitgiven %&gt;% group_by(habit) %&gt;% summarise(mean_weight = mean(weight)) There is an observed difference, but is this difference statistically significant? In order to answer this question we will conduct a hypothesis test . Write the hypotheses for testing if the average weights of babies born to smoking and non-smoking mothers are different. Are the conditions necessary for conducting simulation based inference satisfied? Explain your reasoning. Run the appropriate hypothesis test, calculate the p-value, and interpret the results in context of the data and the hypothesis test. Construct a 95% confidence interval for the difference between the average weights of babies born to smoking and non-smoking mothers. 14.7.3 Baby weight vs. mothers age In this portion of the analysis we focus on two variables. The first one is maturemom. First, a non-inference task: Determine the age cutoff for younger and mature mothers. Use a method of your choice, and explain how your method works. The other variable of interest is lowbirthweight. Conduct a hypothesis test evaluating whether the proportion of low birth weight babies is higher for mature mothers. State the hypotheses, verify the conditions, run the test and calculate the p-value, and state your conclusion in context of the research question. Use \\(\\alpha = 0.05\\). If you find a significant difference, costruct a confidence interval, at the equivalent level to the hypothesis test, for the difference between the proportions of low birth weight babies between mature and younger moms, and interpret this interval in context of the data. "],["lab-12-work-on-projects.html", "15 Lab 12 - Work on projects", " 15 Lab 12 - Work on projects This week youll be working on your projects. Here are a few to do items to get you started. Once you complete these, use the rest of the time to, well, work on your project! Remind yourself of the project assignment Go to the course organization on GitHub and clone your project repo titled project-TEAM_NAME Add your project title and team name to the README.Rmd file in the repo and commit and push your changes. Observe that these are updated in the README of the repo. Open the presentation.Rmd file, knit the document, and review the presentation format. This is where your presentation will go. Update the YAML with your project title, team name, etc. and commit and push your changes. Go to your project repo on GitHub, click on Settings on the top right corner, and scroll down to the section titled GiHub Pages. Under Source, select master branch. This will give you a URL where the website for your project will be automatically built from the content in your README. This might take a few minutes. Click on the link to confirm that the website has been built. (Optional) Once the website it build, you can change its theme using the Theme Chooser. Also, once the website is built, youll need to pull changes to your project in RStudio. Take a look at your rendered project website. Click on the link in the presentation section and you should be able to view the rendered slides. This is the link we will use to project your slides during the presentations. On your repo you should see a text on top No description, website, or topics provided.. Next to it theres an Edit button. Add a short description as well as the URL of your project website here. Note: This website is public, but your repository will remain private,unless you as a team decide you would like to feature your repos in your personal GitHub profiles. If so, I will help you convert your repo to a public repo at the end of the semester. I will not add any marks to your repos so that your public work wont contain your score for the project. Add your dataset to the data folder and add your codebook to the README inthat folder. If in your proposal you were advised to update your codebook, make sure to make those updates. If you had R scripts you used to scrape your data, add them to this folder as well. Add the content from your proposal to the proposal.Rmd file in the proposal folder. Knit the document to make sure everything works and commit and push your proposal to your project repo. Important: Your data now lives in a folder called data that is not inside your proposal folder. So you need to specify the path to your data with \"../data/name_of_datafile\" in your read_csv() (or similar) function. You dont need to make further updates to your proposal at this point, even if your plans for the project change slightly. Load your data in your presentation.Rmd, knit, and make sure everything works. Commit and push your updated proposal to your project repo. Important: Same note as above! Your data now lives in a folder called data that is not inside your presentation folder. So you need to specify the path to your data with \"../data/name_of_datafile\" in your read_csv() (or similar) function. Now that all the logistical details are done, start working on your project. Open issues for things you want to accomplish. Assign them to specific team member(s) if you like. And as you complete the tasks, close the issues. You can also use the issues for discussion on the specific tasks. Strongly recommended: Get a hold of a tutor and run your ideas by them. "],["lab-13-collaborating-on-github.html", "16 Lab 13 - Collaborating on GitHub 16.1 GitHub issues 16.2 Project progress", " 16 Lab 13 - Collaborating on GitHub This week youll continue working on your projects. The first half of the workshop is structured, and you can use the second half to make progress on your projects. 16.1 GitHub issues Issues are a great way to keep track of tasks, enhancements, and bugs for your projects. Theyre kind of like emailexcept they can be shared and discussed with the rest of your team. You can use issues as to-do lists as well as a place for brainstorming / discussing ideas. 16.1.1 Opening an issue Go to your project repo and open a new issue titled Practice issue. Add the following text to the issue: This is not a real issue. This is just some placeholder text. And the following is a bulleted to-do list: - [ ] Do this - [ ] Then that - [ ] And finally this Hit preview to make sure the issue looks like the following: Submit the issue. Then, assign the issue to one or few members of the team. 16.1.2 Working on the issue As you work on the issue you can check the boxes. Note that this will also show progress on the issue on the issue dashboard. Check some of the boxes on your practice issue and confirm that you can see the progress result on the issue dashboard. 16.1.3 Closing the issue Once youre done with an issue, you should close it. You can do this in one of two ways: on GitHub by clicking on Close issue or via a commit that directly addresses the issue. Well practice the second one. If you preface your commits with Fixes, Fixed, Fix, Closes, Closed, or Close, the issue will be closed when you push the changes to your repo. Take a note of the issue number, which will show up next to the issue title. Go to your project on RStudio and make a change. This can be something silly like adding a new line to the issue README. Then commit this change. In your commit message, use one of the special words listed above and reference the issue. For example, if the change I made was to add a new line to the README I would say something like the following: Add a new line to the README, closes #2 Push your changes and observe that the issue is now closed on GitHub. Click on the referenced commit to confirm that it was your last commit that closed the issue. 16.2 Project progress Now back to your project Crafting your to-do list: Discuss your plan for your project as a team, and open at least n issues, where n is the number of students in your team. Not every issue needs to have a checklist, but you might want to include checklists in some of them to remind yourselves the exact steps you discussed to tackle the issue. Then assign at least one issue to each team member. Customizing your website theme: We attempted this last week, and failed due to permission issues. Lets try it one more time! Browse the possible GitHub themes demo pages at the following links. architect cayman dinky hacker leap-day merlot midnight minimal modernist slate tactile time machine Once you decide which theme you prefer (and its perfectly fine if its the default theme you had to begin with), go to the _config.yml file in your repo on RStudio and edit the theme name in the _config.yml file. For example, if you were going from cayman to hacker, your diff would look like the following. Once you commit and push this change, give it a couple minutes for the website to rebuild, and confirm that the theme was changed. **Note:** This is an extremely important step as this is the link I will use on the day of your presentation. There will not be time to make push updates once your presentation session starts. Updating your project description: If you have not yet done so, add a brief description, link to your project website, and topics to your project repo. Citing your data: Now is the time to fix up those citations! In your project README there is a link to a resource for properly citing data. Develop a citation for your dataset and add it under the data section using this guidance. If you have questions, ask a tutor for help! Confirming presentation format: Go to the website for your repo and click on the link that should take you to your presentation. Confirm that your latest changes to the presentation are reflected at this link (which means you must have pushed the resulting HTML file along with the Rmd file where you wrote your presentation). Confirming schedules: Go to the schedule for presentations and confirm that all team members can make it at the beginning of the hour youre assigned to present in. Also note that we will not be meeting in the computer lab. Some of you who have an ILA workshop before this class have been assigned to the first hour due to the sheer number of such students with conflicts. I have checked in with the ILA course organizer and have been told that next weeks workshop is revision, and the presentations in IDS should take priority. So please make sure to leave your workshop by 11:30 at the latest to get to Kings Buildings in time for the presentations. Order within each hour will be announced on the day of the presentations, so you should all be ready to go at the beginning of the hour. Tidying up your coding style: Go to the pull requests tab and take a look at the code styling suggestions. Implement them in the relevant files. Styling suggestions are generated daily at 13:30, so if you do more work today, there may be more suggestions tomorrow. Make sure to check these before you finalize work on your repo. Strongly recommended: Get a hold of a tutor and run your ideas by them. "],["references.html", "References", " References "]]
